{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors: Aishwarya Mathew, Vikram Yabannavar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAMIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Kaggle: [https://www.kaggle.com/](https://www.kaggle.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science is all around us. It is an interdisciplinary field of study that strives to discover meaning behind data. Finding a good first data science project to work on is a hard task. You may know a lot of data science concepts but you get stuck when you want to apply your skills to real world tasks. However, you've come to the right place! Have you ever wanted to make an application that could distinguish between some bad thing and a good thing? This tutorial will do just that. It's actually a basic introduction to the world of data science. This project will teach you how to perform text classification by creating a spam classifier/filter that will distinguish between spam vs. not spam (we call this ham) SMS messages and emails. We will take you through the entire data science lifecycle which includes data collection, data processing, exploratory data analysis and visualization, analysis, hypothesis testing, machine learning and insight/policy decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Content\n",
    "\n",
    "This tutorial will go over the following topics:\n",
    "\n",
    "1. [Installing Libraries](#libraries)\n",
    "<br>\n",
    "2. [Loading Data](#load data)\n",
    "<br>\n",
    "3. [Processing Data](#process data)\n",
    "<br>\n",
    "4. [Exploratory Analysis and Data Visualization](#eda)\n",
    "<br>\n",
    "   4.1. [Getting The Top 100 Words in Spam Messages](#spam1)\n",
    "<br>\n",
    "   4.2. [Getting The Top 100 Words in Ham Messages](#spam1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9c83309c7dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load data'></a>\n",
    "## Loading Data (Data Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of any data science project is data collection. Kaggle is a dataset website that has a lot of real world data. To get started, you need to click on this link, https://www.kaggle.com/uciml/sms-spam-collection-dataset , to download the SMS spam vs. ham dataset from Kaggle to your local disk. You will have to create a user account on Kaggle to download any of their datasets. For email data, we will use the dataset provided from Andrew Ng's Machine Learning course at Stanford. That data can be found [here](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex6materials/ex6DataEmails.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process data'></a>\n",
    "## Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the dataset (a csv file) on your local server, we can start the next step, in which, we prepare our data. The code below is going to export the spam vs. ham dataset from the local server to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam or Ham</th>\n",
       "      <th>SMS Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Spam or Ham                                        SMS Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data into a pandas dataframe\n",
    "spamham_data = pd.read_csv(\"sms.csv\", encoding='latin-1')\n",
    "\n",
    "#removing unnecessary columns\n",
    "del spamham_data['Unnamed: 2']\n",
    "del spamham_data['Unnamed: 3']\n",
    "del spamham_data['Unnamed: 4']\n",
    "#renaming the remaining two columns\n",
    "spamham_data.columns = ['Spam or Ham','SMS Message']\n",
    "\n",
    "#the resulting dataframe with our data\n",
    "spamham_data.head()\n",
    "\n",
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>center</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vary</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S89</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tonight</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "center       2\n",
       "vary         6\n",
       "S89          1\n",
       "tonight      2\n",
       "CM           1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {} #dictionary of word -> num occurrences\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# We tokenize the words. We strip punctuation but do not convert to lowercase,\n",
    "# so Free, free, and FREE are all going to be different for our purposes.\n",
    "# If a word isn't in the dictionary, add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in spamham_data.iterrows():\n",
    "    if row['Spam or Ham'] == 'spam': \n",
    "        line = row['SMS Message'].translate(translator) #remove punctuation\n",
    "        line = line.split() #convert lowercase and split by space\n",
    "        for word in line:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "                \n",
    "spam_word_df = pd.DataFrame.from_dict(word_dict,orient='index')\n",
    "spam_word_df.columns = ['count']\n",
    "spam_word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to            608\n",
       "a             358\n",
       "call          189\n",
       "your          187\n",
       "or            185\n",
       "you           185\n",
       "the           178\n",
       "2             173\n",
       "for           170\n",
       "is            149\n",
       "on            138\n",
       "Call          137\n",
       "now           131\n",
       "have          128\n",
       "and           119\n",
       "4             119\n",
       "from          116\n",
       "FREE          112\n",
       "ur            107\n",
       "with          102\n",
       "mobile         95\n",
       "of             93\n",
       "U              85\n",
       "claim          78\n",
       "are            77\n",
       "You            77\n",
       "our            76\n",
       "prize          73\n",
       "To             73\n",
       "text           72\n",
       "             ... \n",
       "å£1000         35\n",
       "Please         34\n",
       "by             34\n",
       "1              33\n",
       "phone          33\n",
       "Get            33\n",
       "draw           33\n",
       "line           33\n",
       "every          32\n",
       "Claim          32\n",
       "150ppm         32\n",
       "å£2000         31\n",
       "shows          31\n",
       "has            30\n",
       "receive        30\n",
       "Just           30\n",
       "me             29\n",
       "TCs            29\n",
       "Mobile         28\n",
       "PO             28\n",
       "win            28\n",
       "number         28\n",
       "å£150          27\n",
       "apply          27\n",
       "collection     26\n",
       "latest         26\n",
       "1st            26\n",
       "guaranteed     26\n",
       "message        26\n",
       "tone           26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_word_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Ham Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>center</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tonight</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifeis</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidid</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "center       2\n",
       "tonight     57\n",
       "market       3\n",
       "lifeis       1\n",
       "hidid        1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {} #dictionary of word -> num occurrences\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# Loop through the above data frame and check if message is spam. \n",
    "# If it is, we strip punctuation, convert each message to lower case, \n",
    "# then split each message by spaces. \n",
    "# If a word isn't in dictionary, we add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in spamham_data.iterrows():\n",
    "    if row['Spam or Ham'] == 'ham': \n",
    "        line = row['SMS Message'].translate(translator) #remove punctuation\n",
    "        line = line.lower().split() #convert lowercase and split by space\n",
    "        for word in line:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "\n",
    "ham_word_df = pd.DataFrame.from_dict(word_dict,orient='index')\n",
    "ham_word_df.columns = ['count']\n",
    "ham_word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 most common words in non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i        2185\n",
       "you      1837\n",
       "to       1554\n",
       "the      1118\n",
       "a        1052\n",
       "u         972\n",
       "and       848\n",
       "in        811\n",
       "me        756\n",
       "my        743\n",
       "is        728\n",
       "it        590\n",
       "of        524\n",
       "for       501\n",
       "that      486\n",
       "im        449\n",
       "have      438\n",
       "but       418\n",
       "your      414\n",
       "so        412\n",
       "are       409\n",
       "not       406\n",
       "on        391\n",
       "at        377\n",
       "do        377\n",
       "can       376\n",
       "if        347\n",
       "will      334\n",
       "be        332\n",
       "2         305\n",
       "         ... \n",
       "home      160\n",
       "about     159\n",
       "need      156\n",
       "sorry     153\n",
       "from      150\n",
       "as        146\n",
       "still     146\n",
       "see       137\n",
       "by        135\n",
       "n         134\n",
       "later     134\n",
       "r         131\n",
       "only      131\n",
       "da        131\n",
       "she       130\n",
       "back      129\n",
       "think     128\n",
       "well      126\n",
       "today     125\n",
       "send      123\n",
       "tell      121\n",
       "cant      118\n",
       "ì         117\n",
       "hi        117\n",
       "did       116\n",
       "her       113\n",
       "some      112\n",
       "much      112\n",
       "take      112\n",
       "oh        111\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_word_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Words Only In Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting sets of all words in each Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ham_set = set([ line for line in ham_word_df.index])\n",
    "spam_set = set([ line for line in spam_word_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing set difference to get the words found in spam and not in ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_spam_set = spam_set.difference(ham_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vary</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polyphonic</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S89</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bloomberg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mre</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449071512431</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AREA</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62220Cncl</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Here</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mjzgroup</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Costå£150pm</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bergkamp</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTERFLORA</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ree</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6986618</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09077818151</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PO19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Im</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holder</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barbie</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LADIES</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luv</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Try</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apply2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yours</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KL341</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09061743386</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20M12AQ</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOKIA</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08081263000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barkleys</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wings</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Remove</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCM1896WC1N3XX</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FreeMSG</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRAZYIN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parties</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRAVEL</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lots</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07008009200</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09065171142stopsms08</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone750</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnSub</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83332Please</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAM</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wwwmusictrivianet</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logopic</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lastest</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euro</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CallFREEFONE</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260305</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2809 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count\n",
       "vary                      6\n",
       "Polyphonic                1\n",
       "S89                       1\n",
       "CM                        1\n",
       "6031                      2\n",
       "Bloomberg                 2\n",
       "mre                       1\n",
       "449071512431              1\n",
       "AREA                      1\n",
       "62220Cncl                 1\n",
       "Here                      1\n",
       "mjzgroup                  1\n",
       "Costå£150pm               6\n",
       "Bergkamp                  1\n",
       "INTERFLORA                1\n",
       "ree                       1\n",
       "6986618                   2\n",
       "09077818151               1\n",
       "PO19                      1\n",
       "Im                       12\n",
       "Holder                    5\n",
       "Barbie                    1\n",
       "LADIES                    1\n",
       "Luv                       5\n",
       "Try                       2\n",
       "apply2                    1\n",
       "Yours                     2\n",
       "KL341                     2\n",
       "09061743386               2\n",
       "20M12AQ                   1\n",
       "...                     ...\n",
       "NOKIA                    15\n",
       "08081263000               1\n",
       "Barkleys                  1\n",
       "Wings                     1\n",
       "Remove                    2\n",
       "BCM1896WC1N3XX            2\n",
       "ac                        4\n",
       "Virgin                    1\n",
       "FreeMSG                   1\n",
       "CRAZYIN                   1\n",
       "parties                   1\n",
       "GRAVEL                    1\n",
       "Lots                      1\n",
       "07008009200               1\n",
       "09065171142stopsms08      1\n",
       "java                      5\n",
       "phone750                  1\n",
       "UnSub                     1\n",
       "83332Please               1\n",
       "ADAM                      2\n",
       "wwwmusictrivianet         1\n",
       "326                       2\n",
       "logopic                   3\n",
       "lastest                   1\n",
       "Euro                      1\n",
       "Digital                   6\n",
       "CallFREEFONE              2\n",
       "260305                    1\n",
       "To                       73\n",
       "542                       4\n",
       "\n",
       "[2809 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This only contains the words used by spammers\n",
    "only_spam_df = spam_word_df.ix[only_spam_set]\n",
    "only_spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 most common words in Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call           137\n",
       "FREE           112\n",
       "U               85\n",
       "claim           78\n",
       "You             77\n",
       "prize           73\n",
       "To              73\n",
       "Your            71\n",
       "Txt             70\n",
       "STOP            62\n",
       "won             49\n",
       "Nokia           46\n",
       "NOW             44\n",
       "18              43\n",
       "Free            42\n",
       "Reply           42\n",
       "URGENT          41\n",
       "This            40\n",
       "Text            40\n",
       "I               39\n",
       "No              38\n",
       "awarded         37\n",
       "We              36\n",
       "å£1000          35\n",
       "Please          34\n",
       "Get             33\n",
       "Claim           32\n",
       "150ppm          32\n",
       "å£2000          31\n",
       "Just            30\n",
       "              ... \n",
       "TsCs            17\n",
       "vouchers        17\n",
       "Expires         17\n",
       "tones           17\n",
       "12hrs           17\n",
       "Urgent          17\n",
       "Account         16\n",
       "C               16\n",
       "å£250           16\n",
       "Identifier      16\n",
       "Statement       16\n",
       "unredeemed      16\n",
       "08000930705     16\n",
       "Dear            16\n",
       "PRIVATE         16\n",
       "Bonus           16\n",
       "unsubscribe     15\n",
       "Cost            15\n",
       "MobileUpd8      15\n",
       "08000839402     15\n",
       "Had             15\n",
       "YES             15\n",
       "New             15\n",
       "dating          15\n",
       "Ur              15\n",
       "Ltd             15\n",
       "operator        15\n",
       "NOKIA           15\n",
       "If              14\n",
       "Now             14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_spam_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these transformations, we can conclude that text messages contain words relating to winning prizes or monetary values/symbols have a high likelihood of being a spam text message. The word 'prize' is the highest occurring word in the spam text messages, with 113 occurrences while the next three words are also related to winning or prizes. There also seem to be a trend of poor spacing or non-phonetic combinations of letters and numbers, which are common signs of spam messaging.  Next we will do a tf-idf analysis to determine how important each word is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at Emails "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using data from Andrew Ng's Machine Learning course, which splits the data into training sets and test sets. Since we aren't training on the data, we've created a script (included in the GitHub repo) that combines all of these into a convenient CSV for reading into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam or Ham</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>great parttime summer job display box credit a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>auto insurance rate too high dear nlpeople m s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>want best economical hunt vacation life want b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>email million million email addresses want mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>amaze world record sex attention warn adult wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>help loan subject re are debt help qualify fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>beat irs payno please read found father unite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>email million million email addresses want mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>per week home computer put free software comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>best better newest hottest interactive adult w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spam</td>\n",
       "      <td>hypnosis money hi name denni thirdgeneration p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>special book offer americana enterprize ever s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>internet finance top earning top investment sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spam</td>\n",
       "      <td>right tool valuable resource rumor are true s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spam</td>\n",
       "      <td>better return federally mandate electric dereg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>life without debt pardon intrusion offence mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spam</td>\n",
       "      <td>released million was released introducing mill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spam</td>\n",
       "      <td>trouble credit history let us help brighten fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spam</td>\n",
       "      <td>rid debt pay off mortgage much less pay off cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>authenticate sender subject bull s eye target...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spam</td>\n",
       "      <td>f r e e b e s t teen hardcore f r e e multipar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spam</td>\n",
       "      <td>email addresses released email address plus bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spam</td>\n",
       "      <td>really cool hot video attention warn adult war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spam</td>\n",
       "      <td>improvements wound care hello name kevin elphi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spam</td>\n",
       "      <td>advertse legal offer smtp great news advertise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spam</td>\n",
       "      <td>free trial membership latest adult technology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spam</td>\n",
       "      <td>market million try free message comply propose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spam</td>\n",
       "      <td>list software worldwide order form address fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spam</td>\n",
       "      <td>credit program guaranteed credit bad credit cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spam</td>\n",
       "      <td>re free hello offer fantastic free access most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>ham</td>\n",
       "      <td>social science conversation analysis harvey sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>ham</td>\n",
       "      <td>tls explore boundary between phonetic phonolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>ham</td>\n",
       "      <td>linguistic association great britain linguisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>ham</td>\n",
       "      <td>available review kpelle dictionary phonology b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>ham</td>\n",
       "      <td>poznan linguistic meet plm nd poznan linguisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>ham</td>\n",
       "      <td>recent title syntax morphology oxford acquisit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>ham</td>\n",
       "      <td>socio anthropological linguistic oxford power ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>ham</td>\n",
       "      <td>aspect eventuality type nominal semantic filip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>ham</td>\n",
       "      <td>conference linguistic literature th annual con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>ham</td>\n",
       "      <td>multimedium language education second internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>ham</td>\n",
       "      <td>book theoretical descriptive linguistics dialo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>ham</td>\n",
       "      <td>semitic language workshop computational approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>ham</td>\n",
       "      <td>comparative germanic syntax workshop call pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>ham</td>\n",
       "      <td>ws discourse relation coling acl august montre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>ham</td>\n",
       "      <td>evaluation parsing systems call participation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>ham</td>\n",
       "      <td>berkeley women language conference announcemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>ham</td>\n",
       "      <td>linguistic typology linguistic typology volume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>ham</td>\n",
       "      <td>learner corpora international symposium comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>ham</td>\n",
       "      <td>sigphon workshop nd call paper third cfp c l l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>ham</td>\n",
       "      <td>labphon registration reminder dear colleague r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>ham</td>\n",
       "      <td>call lasso call paper lasso xxvii th annual me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>ham</td>\n",
       "      <td>claw program claw workshop announcement second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>ham</td>\n",
       "      <td>correct conference announcement southern illin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>ham</td>\n",
       "      <td>aiml final call paper final call papers advanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>ham</td>\n",
       "      <td>understand pragmatic update understanding prag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>ham</td>\n",
       "      <td>conf computerm workshop colingacl computerm wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>ham</td>\n",
       "      <td>semantic bring attention two publication john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>ham</td>\n",
       "      <td>tense mood selection conference syntax semanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>ham</td>\n",
       "      <td>workshop sposs preliminary program sposs sound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>ham</td>\n",
       "      <td>computationally intensive method quantitative ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Spam or Ham                                              Email\n",
       "0          spam  great parttime summer job display box credit a...\n",
       "1          spam  auto insurance rate too high dear nlpeople m s...\n",
       "2          spam  want best economical hunt vacation life want b...\n",
       "3          spam  email million million email addresses want mon...\n",
       "4          spam  amaze world record sex attention warn adult wa...\n",
       "5          spam  help loan subject re are debt help qualify fin...\n",
       "6          spam  beat irs payno please read found father unite ...\n",
       "7          spam  email million million email addresses want mon...\n",
       "8          spam   per week home computer put free software comp...\n",
       "9          spam  best better newest hottest interactive adult w...\n",
       "10         spam  hypnosis money hi name denni thirdgeneration p...\n",
       "11         spam  special book offer americana enterprize ever s...\n",
       "12         spam  internet finance top earning top investment sp...\n",
       "13         spam  right tool valuable resource rumor are true s ...\n",
       "14         spam  better return federally mandate electric dereg...\n",
       "15         spam  life without debt pardon intrusion offence mea...\n",
       "16         spam  released million was released introducing mill...\n",
       "17         spam  trouble credit history let us help brighten fu...\n",
       "18         spam  rid debt pay off mortgage much less pay off cr...\n",
       "19         spam   authenticate sender subject bull s eye target...\n",
       "20         spam  f r e e b e s t teen hardcore f r e e multipar...\n",
       "21         spam  email addresses released email address plus bo...\n",
       "22         spam  really cool hot video attention warn adult war...\n",
       "23         spam  improvements wound care hello name kevin elphi...\n",
       "24         spam  advertse legal offer smtp great news advertise...\n",
       "25         spam  free trial membership latest adult technology ...\n",
       "26         spam  market million try free message comply propose...\n",
       "27         spam  list software worldwide order form address fre...\n",
       "28         spam  credit program guaranteed credit bad credit cr...\n",
       "29         spam  re free hello offer fantastic free access most...\n",
       "..          ...                                                ...\n",
       "930         ham  social science conversation analysis harvey sa...\n",
       "931         ham  tls explore boundary between phonetic phonolog...\n",
       "932         ham  linguistic association great britain linguisti...\n",
       "933         ham  available review kpelle dictionary phonology b...\n",
       "934         ham  poznan linguistic meet plm nd poznan linguisti...\n",
       "935         ham  recent title syntax morphology oxford acquisit...\n",
       "936         ham  socio anthropological linguistic oxford power ...\n",
       "937         ham  aspect eventuality type nominal semantic filip...\n",
       "938         ham  conference linguistic literature th annual con...\n",
       "939         ham  multimedium language education second internat...\n",
       "940         ham  book theoretical descriptive linguistics dialo...\n",
       "941         ham  semitic language workshop computational approa...\n",
       "942         ham  comparative germanic syntax workshop call pape...\n",
       "943         ham  ws discourse relation coling acl august montre...\n",
       "944         ham  evaluation parsing systems call participation ...\n",
       "945         ham  berkeley women language conference announcemen...\n",
       "946         ham  linguistic typology linguistic typology volume...\n",
       "947         ham  learner corpora international symposium comput...\n",
       "948         ham  sigphon workshop nd call paper third cfp c l l...\n",
       "949         ham  labphon registration reminder dear colleague r...\n",
       "950         ham  call lasso call paper lasso xxvii th annual me...\n",
       "951         ham  claw program claw workshop announcement second...\n",
       "952         ham  correct conference announcement southern illin...\n",
       "953         ham  aiml final call paper final call papers advanc...\n",
       "954         ham  understand pragmatic update understanding prag...\n",
       "955         ham  conf computerm workshop colingacl computerm wo...\n",
       "956         ham  semantic bring attention two publication john ...\n",
       "957         ham  tense mood selection conference syntax semanti...\n",
       "958         ham  workshop sposs preliminary program sposs sound...\n",
       "959         ham  computationally intensive method quantitative ...\n",
       "\n",
       "[960 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data into a pandas dataframe\n",
    "email_spamham_data = pd.read_csv(\"email.csv\", encoding='latin-1')\n",
    "\n",
    "#renaming the two columns\n",
    "email_spamham_data.columns = ['Spam or Ham','Email']\n",
    "\n",
    "email_spamham_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>center</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vary</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ambra</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeware</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tonight</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karicohen</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seymour</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outcome</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntr</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wm</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ti</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teenage</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naturopathic</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finances</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idmail</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countries</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspection</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scotlandnet</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limitedtime</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sanford</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ree</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wolf</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cash</th>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pine</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ming</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refine</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lorato</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bahama</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rosa</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eve</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webrider</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsb</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endorse</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idea</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xoxoxoxoxoxo</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agree</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encourage</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gates</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consta</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returns</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claire</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karrask</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fugitive</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utilitynbsp</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bankruptcy</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selfadress</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesale</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fantasy</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evening</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soundblaster</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miamus</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soloman</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoulder</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9508 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "center           27\n",
       "vary              9\n",
       "ambra            16\n",
       "freeware          1\n",
       "tonight           5\n",
       "karicohen         2\n",
       "market          476\n",
       "seymour           1\n",
       "outcome           1\n",
       "ntr               2\n",
       "wm                1\n",
       "ti                4\n",
       "teenage           3\n",
       "naturopathic      1\n",
       "finances          2\n",
       "idmail            1\n",
       "countries         2\n",
       "inspection        1\n",
       "scotlandnet       5\n",
       "limitedtime       1\n",
       "sanford           1\n",
       "ree               7\n",
       "wolf              1\n",
       "cash            233\n",
       "pine              1\n",
       "ming              1\n",
       "word             87\n",
       "refine            2\n",
       "wrong            20\n",
       "lorato            1\n",
       "...             ...\n",
       "bahama            4\n",
       "cg                2\n",
       "cars              2\n",
       "rosa              1\n",
       "eve               1\n",
       "webrider          2\n",
       "tsb               2\n",
       "endorse           4\n",
       "idea             56\n",
       "xoxoxoxoxoxo      1\n",
       "agree            21\n",
       "encourage        10\n",
       "gates             1\n",
       "consta            1\n",
       "returns          16\n",
       "claire            9\n",
       "karrask           1\n",
       "fugitive          1\n",
       "utilitynbsp       2\n",
       "bankruptcy       30\n",
       "selfadress        1\n",
       "wholesale        10\n",
       "fantasy          13\n",
       "evening           2\n",
       "soundblaster      7\n",
       "le                6\n",
       "expenses          1\n",
       "miamus            3\n",
       "soloman           1\n",
       "shoulder          4\n",
       "\n",
       "[9508 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {} #dictionary of word -> num occurrences\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# We tokenize the words. We strip punctuation but do not convert to lowercase,\n",
    "# so Free, free, and FREE are all going to be different for our purposes.\n",
    "# If a word isn't in dictionary, we add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in email_spamham_data.iterrows():\n",
    "    if row['Spam or Ham'] == 'spam': \n",
    "        line = row['Email Body'].translate(translator) #remove punctuation\n",
    "        line = line.split() #convert lowercase and split by space\n",
    "        for word in line:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    " \n",
    "\n",
    "\n",
    "email_spam_df = pd.DataFrame.from_dict(word_dict,orient='index')\n",
    "email_spam_df.columns = ['count']\n",
    "email_spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words in Spam Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "email          1754\n",
       "s              1572\n",
       "order          1502\n",
       "report         1315\n",
       "address        1300\n",
       "our            1183\n",
       "mail           1173\n",
       "program        1046\n",
       "send           1032\n",
       "free            953\n",
       "list            942\n",
       "receive         887\n",
       "money           873\n",
       "name            871\n",
       "d               841\n",
       "business        753\n",
       "one             732\n",
       "work            675\n",
       "com             670\n",
       "nt              662\n",
       "internet        643\n",
       "http            610\n",
       "please          603\n",
       "day             593\n",
       "information     589\n",
       "over            577\n",
       "check           531\n",
       "us              502\n",
       "market          476\n",
       "each            476\n",
       "               ... \n",
       "company         279\n",
       "pay             276\n",
       "below           270\n",
       "hour            270\n",
       "click           267\n",
       "computer        267\n",
       "advertise       265\n",
       "place           261\n",
       "opportunity     260\n",
       "today           259\n",
       "message         258\n",
       "own             257\n",
       "income          256\n",
       "cd              253\n",
       "t               250\n",
       "easy            249\n",
       "within          247\n",
       "file            246\n",
       "world           246\n",
       "help            242\n",
       "live            239\n",
       "must            238\n",
       "m               238\n",
       "show            237\n",
       "cash            233\n",
       "after           233\n",
       "re              232\n",
       "state           232\n",
       "directory       231\n",
       "save            231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_spam_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these transformations, we can conclude that text messages contain words relating to winning prizes or monetary values/symbols have a high likelihood of being a spam text message. The word 'prize' is the highest occurring word in the spam text messages, with 113 occurrences while the next three words are also related to winning or prizes. There also seem to be a trend of poor spacing or non-phonetic combinations of letters and numbers, which are common signs of spam messaging.  Next we will do a tf-idf analysis to determine how important each word is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ca8e7deb751e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpytagcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_tag_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytagcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_tag_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mYOUR_TEXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"A tag cloud is a visual representation for text data, typicallyused to depict keyword metadata on websites, or to visualize free form text.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Aishwarya\\Anaconda3\\lib\\site-packages\\pytagcloud\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSurface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSRCALPHA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msprite\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSprite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollide_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "from pytagcloud import create_tag_image, make_tags\n",
    "from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "YOUR_TEXT = \"A tag cloud is a visual representation for text data, typically\\\n",
    "used to depict keyword metadata on websites, or to visualize free form text.\"\n",
    "\n",
    "tags = make_tags(get_tag_counts(YOUR_TEXT), maxsize=120)\n",
    "\n",
    "create_tag_image(tags, 'cloud_large.png', size=(900, 600), fontname='Lobster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
