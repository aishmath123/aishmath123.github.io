{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors: Aishwarya Mathew, Vikram Yabannavar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAMIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Kaggle: [https://www.kaggle.com/](https://www.kaggle.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science is all around us. It is an interdisciplinary field of study that strives to discover meaning behind data. Finding a good first data science project to work on is a hard task. You may know a lot of data science concepts but you get stuck when you want to apply your skills to real world tasks. However, you've come to the right place! Have you ever wanted to make an application that could distinguish between some bad thing and a good thing? This tutorial will do just that. It's actually a basic introduction to the world of data science. This project will teach you how to perform text classification by creating a spam classifier/filter that will distinguish between spam vs. not spam (we call this ham) SMS messages and emails. We will take you through the entire data science lifecycle which includes data collection, data processing, exploratory data analysis and visualization, analysis, hypothesis testing, machine learning and insight/policy decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Content\n",
    "\n",
    "This tutorial will go over the following topics:\n",
    "\n",
    "1. [Installing Libraries](#libraries)\n",
    "<br>\n",
    "2. [Loading Data](#load data)\n",
    "<br>\n",
    "3. [Processing Data](#process data)\n",
    "<br>\n",
    "4. [Exploratory Analysis and Data Visualization](#eda)\n",
    "<br>\n",
    "   4.1. [Getting The Top 100 Words in Spam Messages](#spam1)\n",
    "<br>\n",
    "   4.2. [Getting The Top 100 Words in Ham Messages](#spam1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>\n",
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import pytagcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load data'></a>\n",
    "## Loading Data (Data Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of any data science project is data collection. Kaggle is a dataset website that has a lot of real world data. To get started, you need to click on this link, https://www.kaggle.com/uciml/sms-spam-collection-dataset , to download the SMS spam vs. ham dataset from Kaggle to your local disk. You will have to create a user account on Kaggle to download any of their datasets. For email data, we will use the dataset provided from Andrew Ng's Machine Learning course at Stanford. That data can be found [here](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex6materials/ex6DataEmails.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process data'></a>\n",
    "## Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the dataset (a csv file) on your local server, we can start the next step, in which, we prepare our data. The code below is going to export the spam vs. ham dataset from the local server to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam or Ham</th>\n",
       "      <th>SMS Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Spam or Ham                                        SMS Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data into a pandas dataframe\n",
    "spamham_data = pd.read_csv(\"sms.csv\", encoding='latin-1')\n",
    "\n",
    "#removing unnecessary columns\n",
    "del spamham_data['Unnamed: 2']\n",
    "del spamham_data['Unnamed: 3']\n",
    "del spamham_data['Unnamed: 4']\n",
    "#renaming the remaining two columns\n",
    "spamham_data.columns = ['Spam or Ham','SMS Message']\n",
    "\n",
    "#the resulting dataframe with our data\n",
    "spamham_data.head()\n",
    "\n",
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>refundedThis</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>save</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REWARD</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "refundedThis      1\n",
       "REAL1             1\n",
       "save              1\n",
       "REWARD            2\n",
       "professional      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {} #dictionary of word -> num occurrences\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# We tokenize the words. We strip punctuation but do not convert to lowercase,\n",
    "# so Free, free, and FREE are all going to be different for our purposes.\n",
    "# If a word isn't in the dictionary, add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in spamham_data.iterrows():\n",
    "    if row['Spam or Ham'] == 'spam': \n",
    "        line = row['SMS Message'].translate(translator) #remove punctuation\n",
    "        line = line.split() #convert lowercase and split by space\n",
    "        for word in line:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "                \n",
    "spam_word_df = pd.DataFrame.from_dict(word_dict,orient='index')\n",
    "spam_word_df.columns = ['count']\n",
    "spam_word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to            608\n",
       "a             358\n",
       "call          189\n",
       "your          187\n",
       "you           185\n",
       "or            185\n",
       "the           178\n",
       "2             173\n",
       "for           170\n",
       "is            149\n",
       "on            138\n",
       "Call          137\n",
       "now           131\n",
       "have          128\n",
       "and           119\n",
       "4             119\n",
       "from          116\n",
       "FREE          112\n",
       "ur            107\n",
       "with          102\n",
       "mobile         95\n",
       "of             93\n",
       "U              85\n",
       "claim          78\n",
       "You            77\n",
       "are            77\n",
       "our            76\n",
       "prize          73\n",
       "To             73\n",
       "text           72\n",
       "             ... \n",
       "å£1000         35\n",
       "Please         34\n",
       "by             34\n",
       "Get            33\n",
       "1              33\n",
       "phone          33\n",
       "line           33\n",
       "draw           33\n",
       "Claim          32\n",
       "150ppm         32\n",
       "every          32\n",
       "å£2000         31\n",
       "shows          31\n",
       "Just           30\n",
       "receive        30\n",
       "has            30\n",
       "TCs            29\n",
       "me             29\n",
       "number         28\n",
       "win            28\n",
       "PO             28\n",
       "Mobile         28\n",
       "å£150          27\n",
       "apply          27\n",
       "award          26\n",
       "guaranteed     26\n",
       "latest         26\n",
       "tone           26\n",
       "at             26\n",
       "1st            26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_word_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Ham Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reflection</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soonc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctors</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returned</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tc</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "reflection      1\n",
       "soonc           1\n",
       "doctors         1\n",
       "returned        3\n",
       "tc              2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {} #dictionary of word -> num occurrences\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# Loop through the above data frame and check if message is spam. \n",
    "# If it is, we strip punctuation, convert each message to lower case, \n",
    "# then split each message by spaces. \n",
    "# If a word isn't in dictionary, we add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in spamham_data.iterrows():\n",
    "    if row['Spam or Ham'] == 'ham': \n",
    "        line = row['SMS Message'].translate(translator) #remove punctuation\n",
    "        line = line.lower().split() #convert lowercase and split by space\n",
    "        for word in line:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "\n",
    "ham_word_df = pd.DataFrame.from_dict(word_dict,orient='index')\n",
    "ham_word_df.columns = ['count']\n",
    "ham_word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 most common words in non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i        2185\n",
       "you      1837\n",
       "to       1554\n",
       "the      1118\n",
       "a        1052\n",
       "u         972\n",
       "and       848\n",
       "in        811\n",
       "me        756\n",
       "my        743\n",
       "is        728\n",
       "it        590\n",
       "of        524\n",
       "for       501\n",
       "that      486\n",
       "im        449\n",
       "have      438\n",
       "but       418\n",
       "your      414\n",
       "so        412\n",
       "are       409\n",
       "not       406\n",
       "on        391\n",
       "do        377\n",
       "at        377\n",
       "can       376\n",
       "if        347\n",
       "will      334\n",
       "be        332\n",
       "2         305\n",
       "         ... \n",
       "home      160\n",
       "about     159\n",
       "need      156\n",
       "sorry     153\n",
       "from      150\n",
       "as        146\n",
       "still     146\n",
       "see       137\n",
       "by        135\n",
       "later     134\n",
       "n         134\n",
       "da        131\n",
       "r         131\n",
       "only      131\n",
       "she       130\n",
       "back      129\n",
       "think     128\n",
       "well      126\n",
       "today     125\n",
       "send      123\n",
       "tell      121\n",
       "cant      118\n",
       "ì         117\n",
       "hi        117\n",
       "did       116\n",
       "her       113\n",
       "take      112\n",
       "much      112\n",
       "some      112\n",
       "here      111\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_word_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Words Only In Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting sets of all words in each Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ham_set = set([ line for line in ham_word_df.index])\n",
    "spam_set = set([ line for line in spam_word_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing set difference to get the words found in spam and not in ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_spam_set = spam_set.difference(ham_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REAL1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCLU</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REWARD</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPAM</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREEMSG</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOB</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cup</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alfie</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specially</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21m</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09064019788</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cs</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoBox1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09066368470</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cumin</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interflora</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87131</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bootydelious</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promotion</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Download</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slower</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Just</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIX</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRed</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recieve</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STOP2stop</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cst</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YES165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TONES2U</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textand</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sexy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0796XXXXXX</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REALITY</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>celeb</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42810</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>To</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0906346330</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SONY</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11mths</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WELL</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>å£100000</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAM</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banned</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>å£150wk</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08712466669</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WAITING</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09061221066</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIT</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latests</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29100</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POLYS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tsunami</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2809 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "REAL1             1\n",
       "INCLU             1\n",
       "REWARD            2\n",
       "SPAM              1\n",
       "professional      1\n",
       "FREEMSG           1\n",
       "MOB               1\n",
       "Cup               3\n",
       "Alfie             1\n",
       "specially         9\n",
       "21m               1\n",
       "09064019788       1\n",
       "Cs               13\n",
       "SEX               2\n",
       "PoBox1            1\n",
       "09066368470       1\n",
       "std               9\n",
       "cumin             1\n",
       "Interflora        1\n",
       "87131             4\n",
       "bootydelious      1\n",
       "Promotion         1\n",
       "Download          1\n",
       "slower            1\n",
       "Just             30\n",
       "Video            10\n",
       "SIX               2\n",
       "CRed              1\n",
       "recieve           1\n",
       "STOP2stop         1\n",
       "...             ...\n",
       "cst               1\n",
       "YES165            1\n",
       "TONES2U           2\n",
       "textand           1\n",
       "Sexy              1\n",
       "AGE               7\n",
       "0796XXXXXX        1\n",
       "REALITY           2\n",
       "celeb             1\n",
       "42810             1\n",
       "To               73\n",
       "animation         1\n",
       "0906346330        1\n",
       "SONY              1\n",
       "11mths            9\n",
       "WELL              2\n",
       "å£100000          2\n",
       "SAM               2\n",
       "banned            1\n",
       "å£150wk           1\n",
       "08712466669       1\n",
       "WAITING           2\n",
       "78                1\n",
       "09061221066       4\n",
       "leading           1\n",
       "HIT               1\n",
       "latests           1\n",
       "29100             1\n",
       "POLYS             1\n",
       "Tsunami           1\n",
       "\n",
       "[2809 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This only contains the words used by spammers\n",
    "only_spam_df = spam_word_df.ix[only_spam_set]\n",
    "only_spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 most common words in Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call           137\n",
       "FREE           112\n",
       "U               85\n",
       "claim           78\n",
       "You             77\n",
       "prize           73\n",
       "To              73\n",
       "Your            71\n",
       "Txt             70\n",
       "STOP            62\n",
       "won             49\n",
       "Nokia           46\n",
       "NOW             44\n",
       "18              43\n",
       "Reply           42\n",
       "Free            42\n",
       "URGENT          41\n",
       "This            40\n",
       "Text            40\n",
       "I               39\n",
       "No              38\n",
       "awarded         37\n",
       "We              36\n",
       "å£1000          35\n",
       "Please          34\n",
       "Get             33\n",
       "Claim           32\n",
       "150ppm          32\n",
       "å£2000          31\n",
       "Just            30\n",
       "              ... \n",
       "Expires         17\n",
       "vouchers        17\n",
       "tones           17\n",
       "Urgent          17\n",
       "Code            17\n",
       "12hrs           17\n",
       "Identifier      16\n",
       "Dear            16\n",
       "Bonus           16\n",
       "Statement       16\n",
       "Account         16\n",
       "unredeemed      16\n",
       "08000930705     16\n",
       "å£250           16\n",
       "C               16\n",
       "PRIVATE         16\n",
       "NOKIA           15\n",
       "Ltd             15\n",
       "Had             15\n",
       "08000839402     15\n",
       "unsubscribe     15\n",
       "operator        15\n",
       "YES             15\n",
       "New             15\n",
       "dating          15\n",
       "Ur              15\n",
       "Cost            15\n",
       "MobileUpd8      15\n",
       "Only            14\n",
       "UK              14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_spam_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these transformations, we can conclude that text messages contain words relating to winning prizes or monetary values/symbols have a high likelihood of being a spam text message. The word 'prize' is the highest occurring word in the spam text messages, with 113 occurrences while the next three words are also related to winning or prizes. There also seem to be a trend of poor spacing or non-phonetic combinations of letters and numbers, which are common signs of spam messaging.  Next we will do a tf-idf analysis to determine how important each word is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Looking at Emails "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using data from Andrew Ng's Machine Learning course, which splits the data into training sets and test sets. Since we aren't training on the data, we've created a script (included in the GitHub repo) that combines all of these into a convenient CSV for reading into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam or Ham</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>great parttime summer job display box credit a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>auto insurance rate too high dear nlpeople m s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>want best economical hunt vacation life want b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>email million million email addresses want mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>amaze world record sex attention warn adult wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Spam or Ham                                              Email\n",
       "0        spam  great parttime summer job display box credit a...\n",
       "1        spam  auto insurance rate too high dear nlpeople m s...\n",
       "2        spam  want best economical hunt vacation life want b...\n",
       "3        spam  email million million email addresses want mon...\n",
       "4        spam  amaze world record sex attention warn adult wa..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data into a pandas dataframe\n",
    "email_spamham_data = pd.read_csv(\"email.csv\", encoding='latin-1')\n",
    "\n",
    "#renaming the two columns\n",
    "email_spamham_data.columns = ['Spam or Ham','Email']\n",
    "\n",
    "email_spamham_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Ham and Spam Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rome</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>external</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photomask</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valref</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarrant</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personalize</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assault</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nwlink</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inglewood</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trixie</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persistant</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>try</th>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limitedtime</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prospective</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xd</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compute</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corte</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mkii</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forth</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returned</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toon</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>current</th>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selfliquidating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truster</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcgregor</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quit</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anonymity</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxxnet</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poll</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>junker</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vow</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camouflage</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extend</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cupboard</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eudora</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunday</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desperately</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jurisdiction</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fulfil</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sluggish</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypnosis</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tha</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telephone</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dominate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrymonialne</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tapes</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eleven</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compensate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sickness</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boundary</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpc</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flight</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9508 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count\n",
       "rome                 1\n",
       "external             4\n",
       "professional        73\n",
       "photomask            7\n",
       "tc                   1\n",
       "valref               1\n",
       "tarrant              9\n",
       "personalize          7\n",
       "assault              1\n",
       "nwlink               1\n",
       "inglewood            1\n",
       "trixie               2\n",
       "persistant           1\n",
       "try                227\n",
       "limitedtime          1\n",
       "prospective         12\n",
       "xd                   2\n",
       "compute              3\n",
       "corte                1\n",
       "mkii                 3\n",
       "forth                3\n",
       "eight                1\n",
       "returned             2\n",
       "toon                 2\n",
       "current             36\n",
       "selfliquidating      1\n",
       "truster              1\n",
       "hop                  9\n",
       "mcgregor             2\n",
       "quit                25\n",
       "...                ...\n",
       "anonymity            2\n",
       "xxxnet               5\n",
       "poll                 3\n",
       "junker               1\n",
       "vow                  1\n",
       "camouflage           2\n",
       "tx                  21\n",
       "extend               5\n",
       "cupboard             1\n",
       "eudora               6\n",
       "sunday              14\n",
       "ask                139\n",
       "desperately          4\n",
       "jurisdiction         2\n",
       "fulfil               1\n",
       "sluggish             1\n",
       "hypnosis             5\n",
       "stack                3\n",
       "tha                  1\n",
       "telephone           53\n",
       "dominate             1\n",
       "matrymonialne        1\n",
       "tapes                1\n",
       "eleven               4\n",
       "compensate           1\n",
       "carry                9\n",
       "sickness             1\n",
       "boundary            11\n",
       "mpc                  2\n",
       "flight               3\n",
       "\n",
       "[9508 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_words = {} #dictionaries of word -> num occurrences\n",
    "ham_words = {}\n",
    "\n",
    "translator= str.maketrans('','',string.punctuation) #for stripping punctuation\n",
    "\n",
    "# We tokenize the words. We strip punctuation but do not convert to lowercase,\n",
    "# so Free, free, and FREE are all going to be different for our purposes.\n",
    "# If a word isn't in dictionary, we add it with default value 1,\n",
    "# else, increment the corresponding count.\n",
    "\n",
    "for item,row in email_spamham_data.iterrows():\n",
    "    line = row['Email'].translate(translator) #remove punctuation\n",
    "    line = line.split() #convert lowercase and split by space\n",
    "    for word in line:\n",
    "        if row['Spam or Ham'] == 'spam':\n",
    "            if word in spam_words:\n",
    "                spam_words[word] = spam_words[word] + 1\n",
    "            else:\n",
    "                spam_words[word] = 1\n",
    "        else: #if ham\n",
    "            if word in ham_words:\n",
    "                ham_words[word] = ham_words[word] + 1\n",
    "            else:\n",
    "                ham_words[word] = 1\n",
    "\n",
    "\n",
    "email_spam_df = pd.DataFrame.from_dict(spam_words,orient='index')\n",
    "email_spam_df.columns = ['count']\n",
    "email_spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words in Spam Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "email          1754\n",
       "s              1572\n",
       "order          1502\n",
       "report         1315\n",
       "address        1300\n",
       "our            1183\n",
       "mail           1173\n",
       "program        1046\n",
       "send           1032\n",
       "free            953\n",
       "list            942\n",
       "receive         887\n",
       "money           873\n",
       "name            871\n",
       "d               841\n",
       "business        753\n",
       "one             732\n",
       "work            675\n",
       "com             670\n",
       "nt              662\n",
       "internet        643\n",
       "http            610\n",
       "please          603\n",
       "day             593\n",
       "information     589\n",
       "over            577\n",
       "check           531\n",
       "us              502\n",
       "web             476\n",
       "each            476\n",
       "               ... \n",
       "company         279\n",
       "pay             276\n",
       "hour            270\n",
       "below           270\n",
       "computer        267\n",
       "click           267\n",
       "advertise       265\n",
       "place           261\n",
       "opportunity     260\n",
       "today           259\n",
       "message         258\n",
       "own             257\n",
       "income          256\n",
       "cd              253\n",
       "t               250\n",
       "easy            249\n",
       "within          247\n",
       "world           246\n",
       "file            246\n",
       "help            242\n",
       "live            239\n",
       "m               238\n",
       "must            238\n",
       "show            237\n",
       "after           233\n",
       "cash            233\n",
       "re              232\n",
       "state           232\n",
       "save            231\n",
       "directory       231\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_spam_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The Top 100 Words in Ham Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>milena</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefere</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>external</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dworkin</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morri</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hana</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberium</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsimplus</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choueka</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>barcelone</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cp</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bestknown</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prospective</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compulog</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forth</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>install</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worthwhile</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inherent</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manitoba</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcgregor</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vowel</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>erickson</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wenchao</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talker</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sssss</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dordrecht</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milton</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animation</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grape</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campidanian</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>descle</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whither</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anonymity</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loyal</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kiuru</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counterfactually</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blache</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handelshajskolen</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>egil</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filosofium</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tony</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoecke</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>busy</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refundable</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cte</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lesley</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrea</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisconsin</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nomenclature</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labiodental</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batusek</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inclination</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usia</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standardizationare</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keren</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sarah</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17305 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         count\n",
       "milena                                       1\n",
       "prefere                                      1\n",
       "external                                     3\n",
       "dworkin                                      4\n",
       "morri                                        3\n",
       "hana                                         2\n",
       "radon                                        1\n",
       "liberium                                     1\n",
       "tsimplus                                     1\n",
       "choueka                                      3\n",
       "barcelone                                    2\n",
       "cp                                           8\n",
       "bestknown                                    1\n",
       "prospective                                  3\n",
       "compulog                                     1\n",
       "forth                                       17\n",
       "install                                      1\n",
       "eight                                        7\n",
       "worthwhile                                   3\n",
       "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx      1\n",
       "inherent                                    22\n",
       "manitoba                                     2\n",
       "mcgregor                                     4\n",
       "vowel                                       82\n",
       "erickson                                     5\n",
       "wenchao                                      1\n",
       "talker                                       1\n",
       "sssss                                        2\n",
       "dordrecht                                    3\n",
       "milton                                       1\n",
       "...                                        ...\n",
       "animation                                    3\n",
       "grape                                        1\n",
       "campidanian                                  3\n",
       "descle                                       1\n",
       "whither                                      2\n",
       "anonymity                                    2\n",
       "loyal                                        1\n",
       "kiuru                                        1\n",
       "counterfactually                             1\n",
       "blache                                       1\n",
       "handelshajskolen                             1\n",
       "egil                                         1\n",
       "filosofium                                   1\n",
       "tony                                        13\n",
       "hoecke                                       1\n",
       "busy                                         6\n",
       "refundable                                   1\n",
       "cte                                          1\n",
       "lesley                                       2\n",
       "andrea                                      11\n",
       "wisconsin                                   12\n",
       "nomenclature                                 1\n",
       "labiodental                                  1\n",
       "batusek                                      1\n",
       "inclination                                  1\n",
       "usia                                         4\n",
       "standardizationare                           1\n",
       "keren                                        1\n",
       "had                                          3\n",
       "sarah                                       12\n",
       "\n",
       "[17305 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_ham_df = pd.DataFrame.from_dict(ham_words,orient='index')\n",
    "email_ham_df.columns = ['count']\n",
    "email_ham_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Most Common Words in Spam Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language         1525\n",
       "university       1268\n",
       "s                 878\n",
       "linguistic        660\n",
       "de                569\n",
       "information       540\n",
       "conference        495\n",
       "workshop          479\n",
       "english           477\n",
       "e                 420\n",
       "email             418\n",
       "one               398\n",
       "paper             395\n",
       "please            371\n",
       "include           368\n",
       "edu               364\n",
       "research          351\n",
       "address           350\n",
       "abstract          340\n",
       "http              335\n",
       "fax               328\n",
       "word              317\n",
       "h                 315\n",
       "papers            315\n",
       "d                 302\n",
       "speech            301\n",
       "submission        283\n",
       "theory            281\n",
       "www               277\n",
       "m                 276\n",
       "                 ... \n",
       "th                183\n",
       "between           181\n",
       "science           178\n",
       "ac                177\n",
       "issue             177\n",
       "example           175\n",
       "two               175\n",
       "present           175\n",
       "available         172\n",
       "list              172\n",
       "grammar           172\n",
       "both              171\n",
       "write             170\n",
       "area              169\n",
       "discussion        168\n",
       "program           168\n",
       "submit            165\n",
       "john              165\n",
       "number            163\n",
       "approach          162\n",
       "provide           160\n",
       "topic             157\n",
       "international     157\n",
       "application       155\n",
       "copy              154\n",
       "discourse         150\n",
       "state             149\n",
       "j                 148\n",
       "order             147\n",
       "la                146\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_ham_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these transformations, we can conclude that text messages contain words relating to winning prizes or monetary values/symbols have a high likelihood of being a spam text message. The word 'prize' is the highest occurring word in the spam text messages, with 113 occurrences while the next three words are also related to winning or prizes. There also seem to be a trend of poor spacing or non-phonetic combinations of letters and numbers, which are common signs of spam messaging.  Next we will do a tf-idf analysis to determine how important each word is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Words Only In Spam Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting sets of all words in each Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ham_set = set([ line for line in email_ham_df.index])\n",
    "spam_set = set([ line for line in email_spam_df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing set difference to get the words found in spam and not in ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_spam_set = spam_set.difference(ham_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>returned</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personalize</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assault</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nwlink</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>limitedtime</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corte</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exciting</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selfliquidating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truster</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foxatastic</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recieve</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marcap</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>karicohen</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speeding</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nine</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halliwell</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmepge</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hollander</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platinum</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goss</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lessly</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adtek</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selffree</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itch</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intensified</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spots</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cgiforeverweb</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generator</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brew</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beryldshannon</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>places</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdr</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simpler</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personalfinance</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shearer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stipulate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eloan</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patel</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similiar</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lotion</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>globalremove</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obliterate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beatthe</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tracker</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handwritten</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nei</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untold</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>localhost</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webscout</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweepstakes</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>junker</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cupboard</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eudora</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sluggish</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sickness</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borrower</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5465 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count\n",
       "returned             2\n",
       "tc                   1\n",
       "personalize          7\n",
       "assault              1\n",
       "nwlink               1\n",
       "limitedtime          1\n",
       "corte                1\n",
       "exciting             2\n",
       "selfliquidating      1\n",
       "truster              1\n",
       "foxatastic           1\n",
       "fuzz                 1\n",
       "recieve             14\n",
       "marcap               1\n",
       "karicohen            2\n",
       "speeding             1\n",
       "nine                 2\n",
       "halliwell            2\n",
       "hmepge               1\n",
       "hollander            7\n",
       "platinum            10\n",
       "goss                 1\n",
       "lessly               2\n",
       "adtek                4\n",
       "selffree             1\n",
       "itch                 1\n",
       "intensified          2\n",
       "spots                1\n",
       "cgiforeverweb        6\n",
       "generator            8\n",
       "...                ...\n",
       "brew                 7\n",
       "beryldshannon        1\n",
       "fi                   1\n",
       "places               3\n",
       "cdr                  2\n",
       "simpler              2\n",
       "personalfinance      1\n",
       "shearer              1\n",
       "stipulate            1\n",
       "eloan                5\n",
       "patel                1\n",
       "similiar             2\n",
       "lotion               1\n",
       "globalremove         1\n",
       "obliterate           1\n",
       "beatthe              1\n",
       "others               1\n",
       "tracker              1\n",
       "handwritten          2\n",
       "nei                  2\n",
       "untold               2\n",
       "localhost            5\n",
       "webscout             1\n",
       "sweepstakes          8\n",
       "junker               1\n",
       "cupboard             1\n",
       "eudora               6\n",
       "sluggish             1\n",
       "sickness             1\n",
       "borrower             3\n",
       "\n",
       "[5465 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This only contains the words used by spammers\n",
    "only_spam_df = email_spam_df.ix[only_spam_set]\n",
    "only_spam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capitalfm      196\n",
       "nbsp           196\n",
       "ffa            183\n",
       "floodgate      150\n",
       "aol            133\n",
       "bonus          119\n",
       "mailing        118\n",
       "investment     118\n",
       "profit         111\n",
       "hundred         95\n",
       "reports         93\n",
       "stealth         82\n",
       "links           82\n",
       "always          75\n",
       "millions        75\n",
       "offshore        73\n",
       "sales           67\n",
       "invest          62\n",
       "tm              60\n",
       "mlm             60\n",
       "toll            56\n",
       "amaze           55\n",
       "recruit         55\n",
       "album           54\n",
       "mailer          52\n",
       "xxx             51\n",
       "spam            49\n",
       "isp             48\n",
       "goldrush        48\n",
       "cent            48\n",
       "              ... \n",
       "infoseek        30\n",
       "millionaire     29\n",
       "expiration      28\n",
       "amazing         28\n",
       "alba            28\n",
       "unsubscribe     28\n",
       "staggering      27\n",
       "spider          27\n",
       "ram             27\n",
       "href            27\n",
       "moneymake       27\n",
       "advertiser      26\n",
       "vanish          26\n",
       "largest         26\n",
       "instant         26\n",
       "descrambler     26\n",
       "wrap            25\n",
       "plans           25\n",
       "webmaster       25\n",
       "estate          25\n",
       "comply          25\n",
       "teen            25\n",
       "dollars         25\n",
       "quit            25\n",
       "downline        25\n",
       "gamble          25\n",
       "retail          25\n",
       "quot            25\n",
       "upgrade         25\n",
       "loader          24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_spam_df['count'].nlargest(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pytagcloud import create_tag_image, make_tags\n",
    "#from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "#YOUR_TEXT = \"A tag cloud is a visual representation for text data, typically\\\n",
    "#used to depict keyword metadata on websites, or to visualize free form text.\"\n",
    "\n",
    "\n",
    "#creating a dictionary of the text\n",
    "#tags = {}\n",
    "\n",
    "#test = spamham_data.loc[spamham_data['Spam or Ham'] == 'spam']['SMS Message']\n",
    "    \n",
    "\n",
    "\n",
    "#tags = make_tags(get_tag_counts(YOUR_TEXT), maxsize=120)\n",
    "\n",
    "#create_tag_image(tags, 'cloud_large.png', size=(900, 600), fontname='Lobster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
